{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Pulse - Data Preprocessing\n",
    "\n",
    "## Data Cleaning and Feature Engineering\n",
    "\n",
    "This notebook handles:\n",
    "- Missing value imputation\n",
    "- Outlier detection and handling\n",
    "- Datetime parsing and temporal feature creation\n",
    "- Derived feature engineering (rush hour, traffic stress levels)\n",
    "- Data quality documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:44:03.872227Z",
     "start_time": "2026-01-08T10:44:03.860652Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path (works in PyCharm and Jupyter)\n",
    "# This solution works regardless of where the notebook is run from\n",
    "current_dir = Path().resolve()\n",
    "# Check if we're in notebooks directory or project root\n",
    "if (current_dir / 'src').exists():\n",
    "    # We're in project root\n",
    "    project_root = current_dir\n",
    "elif (current_dir.parent / 'src').exists():\n",
    "    # We're in notebooks directory, go up one level\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    # Try to find project root by looking for src directory\n",
    "    project_root = current_dir\n",
    "    while project_root != project_root.parent:\n",
    "        if (project_root / 'src').exists():\n",
    "            break\n",
    "        project_root = project_root.parent\n",
    "\n",
    "# Add project root to Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.data_processing import (\n",
    "    load_data,\n",
    "    inspect_data,\n",
    "    handle_missing_values,\n",
    "    handle_outliers,\n",
    "    parse_datetime,\n",
    "    create_rush_hour_feature,\n",
    "    create_traffic_stress_level,\n",
    "    preprocess_pipeline,\n",
    "    load_and_clean_data\n",
    ")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data\n",
    "\n",
    "Load the dataset from the exploration notebook or directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:44:10.491738Z",
     "start_time": "2026-01-08T10:44:10.411985Z"
    }
   },
   "source": [
    "# Load raw data\n",
    "data_path = '../data/raw/Metro_Interstate_Traffic_Volume.csv'\n",
    "\n",
    "try:\n",
    "    df_raw = load_data(data_path)\n",
    "    print(f\"✓ Raw data loaded: {df_raw.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️  Please run 01_data_exploration.ipynb first or ensure data file exists\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded successfully: 48204 rows, 9 columns\n",
      "✓ Raw data loaded: (48204, 9)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Complete Preprocessing Pipeline\n",
    "\n",
    "The preprocessing pipeline handles all cleaning and feature engineering steps automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:44:15.643642Z",
     "start_time": "2026-01-08T10:44:15.286326Z"
    }
   },
   "source": [
    "# Run complete preprocessing pipeline\n",
    "df_processed, preprocessing_report = preprocess_pipeline(\n",
    "    df_raw,\n",
    "    target_column='traffic_volume',\n",
    "    date_column='date_time',\n",
    "    missing_strategy='forward_fill',  # Good for time series data\n",
    "    outlier_method='cap'  # Cap outliers rather than remove\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING DATA PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "============================================================\n",
      "DATA QUALITY REPORT\n",
      "============================================================\n",
      "Shape: 48204 rows × 9 columns\n",
      "\n",
      "Missing Values:\n",
      "  holiday: 48143 (99.87%)\n",
      "\n",
      "Duplicate Rows: 17\n",
      "Memory Usage: 11.71 MB\n",
      "============================================================\n",
      "✓ Parsed datetime column 'date_time' and extracted temporal features\n",
      "\n",
      "Handling missing values using 'forward_fill' strategy...\n",
      "  holiday: 48143 → 0 missing values\n",
      "✓ Capped outliers in 'traffic_volume' at [-4417.00, 10543.00]\n",
      "✓ Created rush hour features\n",
      "✓ Created traffic stress levels:\n",
      "  Low: < 2158\n",
      "  Medium: 2158 - 4586\n",
      "  High: >= 4586\n",
      "============================================================\n",
      "DATA QUALITY REPORT\n",
      "============================================================\n",
      "Shape: 48204 rows × 19 columns\n",
      "\n",
      "Missing Values:\n",
      "\n",
      "Duplicate Rows: 17\n",
      "Memory Usage: 17.52 MB\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING PIPELINE COMPLETE\n",
      "============================================================\n",
      "Initial rows: 48204\n",
      "Final rows: 48204\n",
      "Features created: 10\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Preprocessing Results\n",
    "\n",
    "Check that all features were created correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:44:29.738489Z",
     "start_time": "2026-01-08T10:44:29.720824Z"
    }
   },
   "source": [
    "# Display new features created\n",
    "print(\"New Features Created:\")\n",
    "print(\"=\"*60)\n",
    "new_features = ['year', 'month', 'day', 'hour', 'day_of_week', 'is_weekend',\n",
    "                'is_rush_hour', 'rush_hour_type', 'traffic_stress_level', 'is_congested']\n",
    "\n",
    "for feature in new_features:\n",
    "    if feature in df_processed.columns:\n",
    "        print(f\"✓ {feature}\")\n",
    "        if df_processed[feature].dtype == 'object':\n",
    "            print(f\"    Values: {df_processed[feature].value_counts().to_dict()}\")\n",
    "        else:\n",
    "            print(f\"    Range: {df_processed[feature].min()} - {df_processed[feature].max()}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Features Created:\n",
      "============================================================\n",
      "✓ year\n",
      "    Range: 2012 - 2018\n",
      "✓ month\n",
      "    Range: 1 - 12\n",
      "✓ day\n",
      "    Range: 1 - 31\n",
      "✓ hour\n",
      "    Range: 0 - 23\n",
      "✓ day_of_week\n",
      "    Range: 0 - 6\n",
      "✓ is_weekend\n",
      "    Range: 0 - 1\n",
      "✓ is_rush_hour\n",
      "    Range: 0 - 1\n",
      "✓ rush_hour_type\n",
      "    Values: {'normal': 36147, 'morning_rush': 6177, 'evening_rush': 5880}\n",
      "✓ traffic_stress_level\n",
      "    Values: {'Medium': 16387, 'High': 15910, 'Low': 15907}\n",
      "✓ is_congested\n",
      "    Range: 0 - 1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Processed Data\n",
    "\n",
    "Save the cleaned and processed dataset for use in EDA and ML notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:45:03.671737Z",
     "start_time": "2026-01-08T10:45:03.157235Z"
    }
   },
   "source": [
    "# Save processed data\n",
    "output_path = '../data/processed/traffic_cleaned.csv'\n",
    "df_processed.to_csv(output_path, index=False)\n",
    "print(f\"✓ Processed data saved to: {output_path}\")\n",
    "print(f\"  Shape: {df_processed.shape}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed data saved to: ../data/processed/traffic_cleaned.csv\n",
      "  Shape: (48204, 19)\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
