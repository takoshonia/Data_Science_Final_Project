{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Pulse - Data Preprocessing\n",
    "\n",
    "## Data Cleaning and Feature Engineering\n",
    "\n",
    "This notebook handles:\n",
    "- Missing value imputation\n",
    "- Outlier detection and handling\n",
    "- Datetime parsing and temporal feature creation\n",
    "- Derived feature engineering (rush hour, traffic stress levels)\n",
    "- Data quality documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:44:03.872227Z",
     "start_time": "2026-01-08T10:44:03.860652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path (works in PyCharm and Jupyter)\n",
    "# This solution works regardless of where the notebook is run from\n",
    "current_dir = Path().resolve()\n",
    "# Check if we're in notebooks directory or project root\n",
    "if (current_dir / 'src').exists():\n",
    "    # We're in project root\n",
    "    project_root = current_dir\n",
    "elif (current_dir.parent / 'src').exists():\n",
    "    # We're in notebooks directory, go up one level\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    # Try to find project root by looking for src directory\n",
    "    project_root = current_dir\n",
    "    while project_root != project_root.parent:\n",
    "        if (project_root / 'src').exists():\n",
    "            break\n",
    "        project_root = project_root.parent\n",
    "\n",
    "# Add project root to Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.data_processing import (\n",
    "    load_data,\n",
    "    inspect_data,\n",
    "    handle_missing_values,\n",
    "    handle_outliers,\n",
    "    parse_datetime,\n",
    "    create_rush_hour_feature,\n",
    "    create_traffic_stress_level,\n",
    "    preprocess_pipeline,\n",
    "    load_and_clean_data\n",
    ")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Decisions Documentation\n",
    "\n",
    "This section documents **why** we made specific data cleaning decisions. This documentation is crucial for reproducibility and for future team members to understand the rationale behind each preprocessing step.\n",
    "\n",
    "### 1. Missing Value Handling Strategy: Forward-Fill\n",
    "\n",
    "**Decision**: Use forward-fill (and backward-fill for remaining values) for missing data.\n",
    "\n",
    "**Why this approach?**\n",
    "- **Time Series Nature**: This is temporal data (hourly traffic measurements). Forward-fill preserves the temporal continuity by carrying forward the last known value, which is appropriate for time series where values change gradually.\n",
    "- **Data Characteristics**: The `holiday` column has 99.87% missing values (48,143 out of 48,204 rows). Most days are not holidays, so forward-fill with \"None\" is appropriate.\n",
    "- **Alternative Considered**: We could have used mean/median, but this would introduce artificial values that don't reflect the actual temporal pattern. Dropping rows would lose 99.87% of the data, which is not acceptable.\n",
    "- **Result**: All missing values in `holiday` column were filled, preserving all 48,204 rows.\n",
    "\n",
    "### 2. Outlier Handling: Capping (Winsorization) Instead of Removal\n",
    "\n",
    "**Decision**: Cap outliers at IQR bounds rather than removing outlier rows.\n",
    "\n",
    "**Why this approach?**\n",
    "- **Preserve Data**: Removing outliers would eliminate valid traffic observations (e.g., unusually high traffic during special events, accidents, or weather events). These are real-world scenarios we want our model to learn from.\n",
    "- **IQR Method**: Using Interquartile Range (IQR) with factor 1.5 is a standard statistical approach that identifies values beyond Q1 - 1.5×IQR and Q3 + 1.5×IQR as outliers.\n",
    "- **Capping Bounds**: Traffic volume outliers were capped at [-4417.00, 10543.00]. The negative lower bound is unusual but preserved to maintain data integrity (some sensors may report negative values during calibration or errors).\n",
    "- **Alternative Considered**: Removing outliers would have reduced our dataset size and potentially removed important edge cases that the model should learn to handle.\n",
    "- **Result**: All 48,204 rows preserved; extreme values were clipped to reasonable bounds.\n",
    "\n",
    "### 3. Duplicate Rows: Kept in Dataset\n",
    "\n",
    "**Decision**: Keep duplicate rows (17 duplicates found).\n",
    "\n",
    "**Why this approach?**\n",
    "- **Temporal Validity**: In time series traffic data, identical measurements at different times are valid (e.g., same traffic volume at 2 AM on different days).\n",
    "- **Low Impact**: Only 17 duplicates out of 48,204 rows (0.035%) - removing them would have minimal impact.\n",
    "- **Preserve Information**: These duplicates may represent legitimate repeated patterns in traffic behavior.\n",
    "- **Result**: All rows including duplicates were retained.\n",
    "\n",
    "### 4. Feature Engineering Decisions\n",
    "\n",
    "**Temporal Features**: Extracted year, month, day, hour, day_of_week from datetime to capture seasonality and time-of-day patterns.\n",
    "\n",
    "**Rush Hour Definition**: Defined as 7-9 AM (morning) and 5-7 PM (evening) based on standard traffic patterns. This is a domain-specific decision based on typical commuter behavior.\n",
    "\n",
    "**Traffic Stress Levels**: Created three categories (Low, Medium, High) using quantile-based thresholds:\n",
    "- Low: < 2158 vehicles/hour (bottom third)\n",
    "- Medium: 2158 - 4586 vehicles/hour (middle third)  \n",
    "- High: ≥ 4586 vehicles/hour (top third)\n",
    "\n",
    "This equal-frequency binning ensures balanced class distribution for classification tasks.\n",
    "\n",
    "### Summary of Data Changes\n",
    "\n",
    "| Action | Rows Affected | Reasoning |\n",
    "|--------|---------------|-----------|\n",
    "| Missing value imputation | 48,143 values in `holiday` | Preserve temporal continuity |\n",
    "| Outlier capping | Values outside IQR bounds | Preserve all observations while reducing extreme values |\n",
    "| Duplicate handling | 17 duplicate rows | Keep for temporal validity |\n",
    "| **Final Dataset** | **48,204 rows × 19 columns** | All original rows preserved |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data\n",
    "\n",
    "Load the dataset from the exploration notebook or directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:44:10.491738Z",
     "start_time": "2026-01-08T10:44:10.411985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded successfully: 48204 rows, 9 columns\n",
      "✓ Raw data loaded: (48204, 9)\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "data_path = '../data/raw/Metro_Interstate_Traffic_Volume.csv'\n",
    "\n",
    "try:\n",
    "    df_raw = load_data(data_path)\n",
    "    print(f\"✓ Raw data loaded: {df_raw.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️  Please run 01_data_exploration.ipynb first or ensure data file exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Complete Preprocessing Pipeline\n",
    "\n",
    "The preprocessing pipeline handles all cleaning and feature engineering steps automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:44:15.643642Z",
     "start_time": "2026-01-08T10:44:15.286326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING DATA PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "============================================================\n",
      "DATA QUALITY REPORT\n",
      "============================================================\n",
      "Shape: 48204 rows × 9 columns\n",
      "\n",
      "Missing Values:\n",
      "  holiday: 48143 (99.87%)\n",
      "\n",
      "Duplicate Rows: 17\n",
      "Memory Usage: 11.71 MB\n",
      "============================================================\n",
      "✓ Parsed datetime column 'date_time' and extracted temporal features\n",
      "\n",
      "Handling missing values using 'forward_fill' strategy...\n",
      "  holiday: 48143 → 0 missing values\n",
      "✓ Capped outliers in 'traffic_volume' at [-4417.00, 10543.00]\n",
      "✓ Created rush hour features\n",
      "✓ Created traffic stress levels:\n",
      "  Low: < 2158\n",
      "  Medium: 2158 - 4586\n",
      "  High: >= 4586\n",
      "============================================================\n",
      "DATA QUALITY REPORT\n",
      "============================================================\n",
      "Shape: 48204 rows × 19 columns\n",
      "\n",
      "Missing Values:\n",
      "\n",
      "Duplicate Rows: 17\n",
      "Memory Usage: 17.52 MB\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING PIPELINE COMPLETE\n",
      "============================================================\n",
      "Initial rows: 48204\n",
      "Final rows: 48204\n",
      "Features created: 10\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run complete preprocessing pipeline\n",
    "df_processed, preprocessing_report = preprocess_pipeline(\n",
    "    df_raw,\n",
    "    target_column='traffic_volume',\n",
    "    date_column='date_time',\n",
    "    missing_strategy='forward_fill',  # Good for time series data\n",
    "    outlier_method='cap'  # Cap outliers rather than remove\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Preprocessing Results\n",
    "\n",
    "Check that all features were created correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:44:29.738489Z",
     "start_time": "2026-01-08T10:44:29.720824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Features Created:\n",
      "============================================================\n",
      "✓ year\n",
      "    Range: 2012 - 2018\n",
      "✓ month\n",
      "    Range: 1 - 12\n",
      "✓ day\n",
      "    Range: 1 - 31\n",
      "✓ hour\n",
      "    Range: 0 - 23\n",
      "✓ day_of_week\n",
      "    Range: 0 - 6\n",
      "✓ is_weekend\n",
      "    Range: 0 - 1\n",
      "✓ is_rush_hour\n",
      "    Range: 0 - 1\n",
      "✓ rush_hour_type\n",
      "    Values: {'normal': 36147, 'morning_rush': 6177, 'evening_rush': 5880}\n",
      "✓ traffic_stress_level\n",
      "    Values: {'Medium': 16387, 'High': 15910, 'Low': 15907}\n",
      "✓ is_congested\n",
      "    Range: 0 - 1\n"
     ]
    }
   ],
   "source": [
    "# Display new features created\n",
    "print(\"New Features Created:\")\n",
    "print(\"=\"*60)\n",
    "new_features = ['year', 'month', 'day', 'hour', 'day_of_week', 'is_weekend',\n",
    "                'is_rush_hour', 'rush_hour_type', 'traffic_stress_level', 'is_congested']\n",
    "\n",
    "for feature in new_features:\n",
    "    if feature in df_processed.columns:\n",
    "        print(f\"✓ {feature}\")\n",
    "        if df_processed[feature].dtype == 'object':\n",
    "            print(f\"    Values: {df_processed[feature].value_counts().to_dict()}\")\n",
    "        else:\n",
    "            print(f\"    Range: {df_processed[feature].min()} - {df_processed[feature].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Processed Data\n",
    "\n",
    "Save the cleaned and processed dataset for use in EDA and ML notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:45:03.671737Z",
     "start_time": "2026-01-08T10:45:03.157235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed data saved to: ../data/processed/traffic_cleaned.csv\n",
      "  Shape: (48204, 19)\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "output_path = '../data/processed/traffic_cleaned.csv'\n",
    "df_processed.to_csv(output_path, index=False)\n",
    "print(f\"✓ Processed data saved to: {output_path}\")\n",
    "print(f\"  Shape: {df_processed.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
